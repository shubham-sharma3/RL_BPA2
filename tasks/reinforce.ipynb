{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RLLBC Bonus Point Assignment II Part A\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0, released 2021-06-24</div>\n",
    "For task instructions, refer to the assignment PDF.\n",
    "\n",
    "* The parts of the code you are to implement are indicated via `# TODO` comments.\n",
    "* You can use the `# Test code` cells to verify your implementation. However note that these are not the unit tests used for grading.\n",
    "* Some cells create export file in the `solution/` folder. _Include whole `solution/` folder in your submission_.\n",
    "* DO NOT CLEAR THE OUTPUT of the notebook you are submitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question A2 - Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import gym\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create solution folder\n",
    "Path(\"solution/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note: The next cell is optional, as this will _not_ run on the JupterHub. To render the CartPole environemnt, you need to set up your Jupyter environment locally (see assignment PDF). Rendering is not required for this assignment, but the visualization may help you to understand what your policy is atually learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the environment (Does not run on the JupyterHub)\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "for _ in range(10):\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        state, _, done, _ = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### a) Defining the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # ********************\n",
    "        # TODO Create layers\n",
    "        \n",
    "        self.layer1 = nn.Linear(4,128)\n",
    "        self.layer2 = nn.Linear(128,2)\n",
    "\n",
    "\n",
    "        # ********************\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ********************\n",
    "        # TODO Implement forward pass\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "       # ********************\n",
    "        return F.softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "_test_output = Policy()(torch.tensor([[1.0, 2, 3, 4]]))\n",
    "assert _test_output.shape == (1, 2), f\"Expected output shape (1, 2), got {_test_output.shape}\"\n",
    "np.testing.assert_almost_equal(_test_output.detach().numpy().sum(), 1, err_msg=\"Output is not a probability distribution.\")\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) Action Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_action(probs):\n",
    "    \"\"\"Sample one action from the action distribution of this state.\n",
    "    \n",
    "    Args:\n",
    "        probs: action probabilities\n",
    "\n",
    "    Returns:\n",
    "        action: The sampled action\n",
    "        log_prob: Logarithm of the probability for sampling that action\n",
    "    \"\"\"\n",
    "    # TODO Implement action sampling\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    \n",
    "\n",
    "    return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "_test_action, _test_log_prob = sample_action(torch.tensor([1, 2, 3, 4]))\n",
    "assert _test_action in [0, 1, 2, 3], f\"Invalid action {_test_action}\"\n",
    "np.testing.assert_approx_equal(_test_log_prob, np.log((_test_action + 1) / 10))\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### c) Return Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_return(rewards, gamma=0.99):\n",
    "    \"\"\"Estimate return based of observed rewards\n",
    "    \n",
    "    Args:\n",
    "        rewards: Series of observed rewards\n",
    "        gamma: discount factor\n",
    "    \"\"\"\n",
    "    # TODO Implement return estimation\n",
    "    returns = np.zeros_like(rewards)\n",
    "    rewards_len = len(rewards)\n",
    "    \n",
    "    Gtplus1 = 0\n",
    "    # loop from 9 to 0\n",
    "    for i in reversed(range(rewards_len)):\n",
    "        # check if it is the first iteration\n",
    "        if i == rewards_len - 1:\n",
    "            Gtplus1 = rewards[i]\n",
    "            returns[i] = Gtplus1\n",
    "            continue\n",
    "        \n",
    "        returns[i] = rewards[i] + gamma * Gtplus1\n",
    "        \n",
    "        Gtplus1 = returns[i]\n",
    "        \n",
    "    \n",
    "    returns -= np.mean(returns)\n",
    "    if len(returns) != 1:\n",
    "        \n",
    "        returns/=np.std(returns) \n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "np.testing.assert_array_almost_equal(\n",
    "    estimate_return(np.ones(10), gamma=0.99),\n",
    "    [1.54572815, 1.21139962, 0.87369404, 0.53257729, 0.18801491, -0.16002789, -0.51158628, -0.86669576, -1.22539221, -1.58771185]\n",
    ")\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### d) Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb#ch0000017?line=51'>52</a>\u001b[0m \u001b[39m# ********************\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb#ch0000017?line=52'>53</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb#ch0000017?line=53'>54</a>\u001b[0m \u001b[39m# Update policy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb#ch0000017?line=54'>55</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb#ch0000017?line=55'>56</a>\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb#ch0000017?line=56'>57</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb#ch0000017?line=58'>59</a>\u001b[0m \u001b[39m# Print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "policy = Policy()\n",
    "\n",
    "# Hyperparams\n",
    "episodes = 1000\n",
    "gamma = 0.99\n",
    "learn_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=learn_rate)\n",
    "\n",
    "total_rewards = []\n",
    "with tqdm(range(episodes)) as pbar:\n",
    "    for _ in pbar:\n",
    "        # Run one episode\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            # Take a step\n",
    "            # ********************\n",
    "            # TODO Sample action for current state\n",
    "            prob = policy.forward(torch.tensor([state]))\n",
    "            \n",
    "            \n",
    "            actionT, log_prob = sample_action(prob.data[0])\n",
    "            # print(type(action))\n",
    "            # print(log_prob)\n",
    "            action = int(actionT)\n",
    "            # log_prob = float(log_prob)\n",
    "\n",
    "            # ********************\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Bookkeeping\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            # print(np.shape(rewards))\n",
    "            \n",
    "        print(np.shape(log_probs))\n",
    "        \n",
    "        total_rewards.append(sum(rewards))\n",
    "        \n",
    "        # ********************\n",
    "        # TODO Compute loss\n",
    "        returns = estimate_return(rewards)\n",
    "        policy_loss = 0\n",
    "        # policy_loss = -log_prob*(estimate_return(rewards))\n",
    "        for l,r in zip(log_probs,returns):\n",
    "            policy_loss += -l*r\n",
    "        \n",
    "\n",
    "        # ********************\n",
    "\n",
    "        # Update policy\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        pbar.set_description(\n",
    "            f\"Mean training reward {np.mean(total_rewards[-100:]):.02f}\")\n",
    "\n",
    "# Save model\n",
    "with open(\"solution/a2d.pt\", \"wb\") as f:\n",
    "    torch.save(policy, f)\n",
    "\n",
    "# Plot training\n",
    "plt.plot(total_rewards, label=\"per episode\")\n",
    "plt.plot(pd.DataFrame(total_rewards).rolling(\n",
    "    100).mean(), label=\"average reward\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/a2d.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 100/100 [00:00<00:00, 256.01it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Average reward below 487.5, got 22.99",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb#ch0000017?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m env\u001b[39m.\u001b[39m_elapsed_steps\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb#ch0000017?line=14'>15</a>\u001b[0m _avg_reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([_rollout(seed\u001b[39m=\u001b[39mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidating\u001b[39m\u001b[39m\"\u001b[39m)])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb#ch0000017?line=15'>16</a>\u001b[0m \u001b[39massert\u001b[39;00m _avg_reward \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m487.5\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAverage reward below 487.5, got \u001b[39m\u001b[39m{\u001b[39;00m_avg_reward\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/reinforce.ipynb#ch0000017?line=16'>17</a>\u001b[0m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mok (Average reward \u001b[39m\u001b[39m{\u001b[39;00m_avg_reward\u001b[39m:\u001b[39;00m\u001b[39m0.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Average reward below 487.5, got 22.99"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "policy.eval()  # Switch to evaluation mode\n",
    "\n",
    "\n",
    "def _rollout(seed):\n",
    "    env.seed(seed)\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "        action = np.argmax(probs.detach().numpy())  # Greedy action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "    return env._elapsed_steps\n",
    "\n",
    "\n",
    "_avg_reward = np.mean([_rollout(seed=i) for i in tqdm(range(100), desc=\"Validating\")])\n",
    "assert _avg_reward >= 487.5, f\"Average reward below 487.5, got {_avg_reward}\"\n",
    "f\"ok (Average reward {_avg_reward:0.2f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the learned policy (this will not run on the JupyterHub)\n",
    "greedy = True\n",
    "\n",
    "policy.eval()  # Switch to evaluation mode\n",
    "state, done = env.reset(), False\n",
    "while not done:\n",
    "    probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "    if greedy:\n",
    "        action = np.argmax(probs.detach().numpy())  # Chose optimal action\n",
    "    else:\n",
    "        action = sample_action(probs)[0]  # Sample from action distribution\n",
    "    state, _, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rllbc_bpa2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b12679fe4722524eeb4ab13231b185eda78a2ffc564a839f2b0a0233119771d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
