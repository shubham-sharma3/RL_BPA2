{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RLLBC Bonus Point Assignment II Part C\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0, released 2022-06-24</div>\n",
    "For task instructions, refer to the assignment PDF.\n",
    "\n",
    "* The parts of the code you are to implement are indicated via `# TODO` comments.\n",
    "* You can use the `# Test code` cells to verify your implementation. However note that these are not the unit tests used for grading.\n",
    "* Some cells create export file in the `solution/` folder. _Include whole `solution/` folder in your submission_.\n",
    "* DO NOT CLEAR THE OUTPUT of the notebook you are submitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "import optparse\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create solution folder\n",
    "Path(\"solution/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Render Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the environment (This will not run on JupyterHub)\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "for _ in range(10):\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        state, _, done, _ = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "            nn.Linear(state_dim, n_latent_var),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_latent_var, n_latent_var),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_latent_var, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "            nn.Linear(state_dim, n_latent_var),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_latent_var, n_latent_var),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(n_latent_var, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        return action.item()\n",
    "\n",
    "    def act(self, state, memory):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(dist.log_prob(action))\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = Categorical(action_probs)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "        state_value = self.value_layer(state)\n",
    "\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Proximal Policy Optimization\n",
    "\n",
    "### a) compute MC estimates\n",
    "### b) compute surrogate loss funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var, lr, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
    "            #--------------------\n",
    "            # TODO compute every-visit MC estimate of state values (variable name: discounted_reward)\n",
    "            #--------------------\n",
    "            while not is_terminal:\n",
    "                rewards.append(discounted_reward + reward)\n",
    "                discounted_reward = np.sum(rewards)/len(rewards)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(memory.states).to(device).detach()\n",
    "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
    "        old_logprobs = torch.stack(memory.logprobs).to(device).detach()\n",
    "\n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()\n",
    "            #--------------------\n",
    "            # TODO implement the two surrogate loss functions of formula (7) in the PPO paper\n",
    "            #--------------------\n",
    "            surr1 = np.sum(ratios*advantages)\n",
    "            clip = np.clip(ratios,1-self.eps_clip,1+self.eps_clip)\n",
    "            surr2 = np.sum(clip*advantages)\n",
    "            \n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### e) Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\"\n",
    "# creating environment\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "render = False\n",
    "solved_reward = 230  # stop training if avg_reward > solved_reward\n",
    "log_interval = 20  # print avg reward in the interval\n",
    "max_episodes = 2000  # max training episodes\n",
    "max_timesteps = 300  # max timesteps in one episode\n",
    "n_latent_var = 64  # number of variables in hidden layer\n",
    "update_timestep = 2000  # update policy every n timesteps\n",
    "lr = 0.002\n",
    "gamma = 0.99  # discount factor\n",
    "K_epochs = 4  # update policy for K epochs\n",
    "eps_clip = 0.2  # clip parameter for PPO\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training Loop\n",
    "### c) Data collection\n",
    "### d) Policy update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/gym/core.py:200: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LunarLander-v2 Clipping: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean training reward -0.03: 100%|█████████▉| 1999/2000 [00:08<00:00, 230.28it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "update() missing 1 required positional argument: 'memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/ppo.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/ppo.ipynb#ch0000013?line=41'>42</a>\u001b[0m \u001b[39m# update if its time\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/ppo.ipynb#ch0000013?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m timestep \u001b[39m%\u001b[39m update_timestep \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/ppo.ipynb#ch0000013?line=43'>44</a>\u001b[0m     \u001b[39m#--------------------\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/ppo.ipynb#ch0000013?line=44'>45</a>\u001b[0m     \u001b[39m#TODO update PPO policy, clear memory afterwards\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/ppo.ipynb#ch0000013?line=45'>46</a>\u001b[0m     PPO\u001b[39m.\u001b[39;49mupdate(memory)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/ppo.ipynb#ch0000013?line=46'>47</a>\u001b[0m     memory\u001b[39m.\u001b[39mclear_memory()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/ppo.ipynb#ch0000013?line=47'>48</a>\u001b[0m     \u001b[39m#--------------------\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: update() missing 1 required positional argument: 'memory'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "env.seed(random_seed)\n",
    "\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, n_latent_var, lr, gamma, K_epochs, eps_clip)\n",
    "print(env_name, \"Clipping:\", eps_clip)\n",
    "\n",
    "# logging variables\n",
    "rewards = []\n",
    "lengths = []\n",
    "timestep = 0\n",
    "def save_statistics():\n",
    "    with open(f\"./solution/PPO_{env_name}-eps{eps_clip}-k{K_epochs}-s{random_seed}-stat.pkl\", 'wb') as f:\n",
    "        pickle.dump({\"rewards\": rewards, \"lengths\": lengths, \"eps\": eps_clip, \"epochs\": K_epochs}, f)\n",
    "# training loop\n",
    "with tqdm(range(1, max_episodes + 1)) as pbar:\n",
    "    for i_episode in pbar:\n",
    "        state = env.reset()\n",
    "        running_reward = 0\n",
    "        for t in range(max_timesteps):\n",
    "            timestep += 1\n",
    "\n",
    "            # Running policy_old:\n",
    "            #--------------------\n",
    "            #TODO interact with the environment using policy_old and store the transitions and whether the rollout\n",
    "            # terminated as well as the log probabilities in the memory.\n",
    "            #--------------------\n",
    "            next_state = state\n",
    "            actorCritic = ActorCritic(state_dim, action_dim, n_latent_var)\n",
    "            action  = actorCritic.forward(state =next_state)\n",
    "            memory.states.append(state)\n",
    "            log_prob,_,_ = actorCritic.evaluate(torch.from_numpy(state),torch.as_tensor(action))\n",
    "            next_state, reward, is_terminal,_ = env.step(action)\n",
    "            \n",
    "            memory.states.append(next_state)\n",
    "            memory.actions.append(action)\n",
    "            memory.logprobs.append(log_prob)\n",
    "            memory.rewards.append(reward)\n",
    "            memory.is_terminals.append(is_terminal)\n",
    "            \n",
    "\n",
    "            # update if its time\n",
    "            if timestep % update_timestep == 0:\n",
    "                #--------------------\n",
    "                #TODO update PPO policy, clear memory afterwards\n",
    "                PPO.update(memory)\n",
    "                memory.clear_memory()\n",
    "                #--------------------\n",
    "\n",
    "            running_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(running_reward)\n",
    "        lengths.append(t)\n",
    "        pbar.set_description(f\"Mean training reward {np.mean(rewards[-log_interval:]):.02f}\")\n",
    "        if np.mean(rewards[-log_interval:]) >= solved_reward:\n",
    "            break  # Stop training\n",
    "                \n",
    "save_statistics()\n",
    "torch.save(ppo.policy.state_dict(), f'./solution/PPO_{env_name}-eps{eps_clip}.pth')\n",
    "\n",
    "# Plot training\n",
    "plt.plot(rewards, label=\"per episode\")\n",
    "plt.plot(pd.DataFrame(rewards).rolling(100).mean(), label=\"average reward\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/ppo.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c6966d41f34f0ba5a49c9a602e8005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ok (Average reward 208.99)'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = \"LunarLander-v2\"\n",
    "# creating environment\n",
    "env = gym.make(env_name)\n",
    "ppo.policy.eval()#switch to evaluation mode\n",
    "def _rollout(seed):\n",
    "    running_reward = 0.0\n",
    "    env.seed=seed\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        action = ppo.policy.forward(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        running_reward += reward\n",
    "    return running_reward\n",
    "\n",
    "\n",
    "_avg_return = np.mean([_rollout(seed=i) for i in tqdm(range(100), desc=\"Validating\")])\n",
    "assert _avg_return >=200 , f\"Average reward below 50, got {_avg_return}\"\n",
    "f\"ok (Average reward {_avg_return:0.2f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the learned policy (this will not run on the JupyterHub)\n",
    "env_name = \"LunarLander-v2\"\n",
    "# creating environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "for i in range(10):\n",
    "    env.seed(i)\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        action = ppo.policy.forward(state)\n",
    "        state, _, done, _ = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rllbc_bpa2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b12679fe4722524eeb4ab13231b185eda78a2ffc564a839f2b0a0233119771d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
