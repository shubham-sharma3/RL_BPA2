{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RLLBC Bonus Point Assignment II Part B\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0, released 2022-06-24</div>\n",
    "For task instructions, refer to the assignment PDF.\n",
    "\n",
    "* The parts of the code you are to implement are indicated via `# TODO` comments.\n",
    "* You can use the `# Test code` cells to verify your implementation. However note that these are not the unit tests used for grading.\n",
    "* Some cells create export file in the `solution/` folder. _Include whole `solution/` folder in your submission_.\n",
    "* DO NOT CLEAR THE OUTPUT of the notebook you are submitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gym\n",
    "from tqdm.auto import tqdm\n",
    "rng = np.random.RandomState(1234)\n",
    "\n",
    "# Create solution folder\n",
    "Path(\"solution/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question B1 - Deep Q-Networks\n",
    "### a) Implement Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, mem_size, state_shape):\n",
    "        \"\"\"Initialization of the replay buffer.\n",
    "        \n",
    "        The memories have the following data types:\n",
    "            states: float32\n",
    "            next_states: float32\n",
    "            actions: int64\n",
    "            rewards: float32\n",
    "            is_terminal: bool\n",
    "\n",
    "        Args:\n",
    "            mem_size: Capacity of this buffer\n",
    "            state_shape: Shape of state and next_state\n",
    "        \"\"\"\n",
    "        self.mem_size = mem_size  # Capacity of the buffer\n",
    "        self.mem_cntr = 0         # Number of added elements\n",
    "        self.state_memory = np.zeros((self.mem_size, *state_shape), dtype=np.float32)\n",
    "        self.next_state_memory = np.zeros((self.mem_size, *state_shape), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        self.state_shape = state_shape\n",
    "    \n",
    "    def is_filled(self):\n",
    "        \"\"\"Check if the memory is filled.\"\"\"\n",
    "        return self.mem_cntr >= self.mem_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, is_terminal):\n",
    "        \"\"\"Add one transition to the buffer.\n",
    "\n",
    "        Replaces the oldest transition in memory.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        i = self.mem_cntr % self.mem_size\n",
    "        \n",
    "        self.state_memory[i] = state\n",
    "        self.next_state_memory[i] = next_state\n",
    "        self.action_memory[i] = action\n",
    "        self.reward_memory[i] = reward\n",
    "        self.terminal_memory[i] = is_terminal\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\"Sample one batch from the memory.\"\"\"\n",
    "        # TODO\n",
    "        idx = []\n",
    "        for i in range(batch_size):\n",
    "            idx.append(rng.randint(0,self.mem_size))\n",
    "        \n",
    "        states = np.zeros((batch_size,*self.state_shape),dtype=np.float32)\n",
    "        actions = np.zeros(batch_size,dtype=np.int64)\n",
    "        next_states = np.zeros((batch_size,*self.state_shape),dtype=np.float32)\n",
    "        rewards = np.zeros(batch_size, dtype=np.float32)\n",
    "        is_terminal = np.zeros(batch_size, dtype=np.bool)\n",
    "        \n",
    "        for i in range(len(idx)):\n",
    "            #index = int(np.random.choice(self.mem_size, 1))\n",
    "            states[i] = self.state_memory[idx[i]]\n",
    "            next_states[i] = self.next_state_memory[idx[i]]\n",
    "            actions[i] = self.action_memory[idx[i]] \n",
    "            rewards[i] = self.reward_memory[idx[i]]\n",
    "            is_terminal[i] = self.terminal_memory[idx[i]]\n",
    "\n",
    "        return states, actions, rewards, next_states, is_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8367/111603329.py:22: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "_buffer = ReplayBuffer(10, (5, ))\n",
    "assert _buffer.mem_size == 10\n",
    "assert _buffer.mem_cntr == 0\n",
    "for i in range(10):  # Fill test values\n",
    "    _buffer.add(np.arange(5) + i, 5 + i, 6 + i, 7 + np.arange(5) + i, (12 + i) % 2 == 0)\n",
    "assert _buffer.mem_size == 10\n",
    "assert _buffer.mem_cntr == 10, \"Wrong mem_cntr\"\n",
    "\n",
    "_is = set()\n",
    "for s, a, r, s_, t in zip(*_buffer.sample_batch(5)):\n",
    "    i = s[0]\n",
    "    assert 0 <= i < 10, \"Wrong states\"\n",
    "    _is.add(i)\n",
    "    np.testing.assert_array_equal(s, np.arange(5) + i, err_msg=\"Wrong states\")\n",
    "    np.testing.assert_equal(a, 5 + i, err_msg=\"Wrong actions\")\n",
    "    np.testing.assert_equal(r, 6 + i, err_msg=\"Wrong rewards\")\n",
    "    np.testing.assert_array_equal(s_, 7 + np.arange(5) + i, err_msg=\"Wrong next states\")\n",
    "    np.testing.assert_equal(t, (12 + i) % 2 == 0, err_msg=\"Wrong terminals\")\n",
    "assert len(_is) == 5, \"Duplicate transitions\"\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) Fill replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n",
      "/tmp/ipykernel_8367/111603329.py:22: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "\n",
    "# Initialize replay buffer\n",
    "buffer = ReplayBuffer(mem_size=25000, state_shape=env.observation_space.shape)\n",
    "\n",
    "# ********************\n",
    "# TODO Sample transitions from environment and add to buffer\n",
    "#next_state, is_terminal = env.reset(), False\n",
    "while not buffer.is_filled():\n",
    "    # Run one episode\n",
    "    next_state, is_terminal = env.reset(), False\n",
    "    while not is_terminal:\n",
    "        # Take a step\n",
    "        # ********************\n",
    "        state = next_state\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, is_terminal, _ = env.step(action) # Achtung Unterstrich\n",
    "        # ********************\n",
    "        \n",
    "        # Add step to buffer\n",
    "        # ********************\n",
    "        buffer.add(state, action, reward, next_state, is_terminal)\n",
    "        # ********************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "assert buffer.is_filled(), f\"Buffer not filled, only {buffer.mem_cntr}/{buffer.mem_size} transitions in memory\"\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question B2 - Deep Q-Networks\n",
    "### a) Define Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1234)\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        # TODO Create layers\n",
    "        # Inputs to hidden layer\n",
    "        self.hidden = nn.Linear(4,128)\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(128,2) # EVentually add layers \n",
    "\n",
    "    def forward(self, state):\n",
    "        # TODO Implement forward pass\n",
    "        x = F.relu(self.hidden(state)) # Different functions?\n",
    "        Q = self.output(x)\n",
    "        #Q = F.softmax(self.output(x),dim=1)\n",
    "\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test code\n",
    "_test_output = DeepQNetwork()(torch.FloatTensor([[1, 2, 3, 4]]))\n",
    "assert _test_output.shape == (1, 2), f\"Expected output shape (1, 2), got {_test_output.shape}\"\n",
    "\"ok\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### b) $\\epsilon$-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, q_network, epsilon=0.05):\n",
    "    \"\"\"Perform epsilon-greedy action sampling.\n",
    "\n",
    "    Args:\n",
    "        state: numpy ndarray, current state\n",
    "        q_network: torch module\n",
    "\n",
    "    Returns:\n",
    "        action: one action\n",
    "    \"\"\"\n",
    "    # TODO Epsilon-greedy action sampling\n",
    "    Q = q_network.forward(torch.tensor([state]))\n",
    "    #Q = q_network.forward(state)\n",
    "    if rng.rand() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = torch.argmax(Q)\n",
    "        action = action.item()\n",
    "\n",
    "    #action = int(action)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8367/2294420513.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1648016052946/work/torch/csrc/utils/tensor_new.cpp:198.)\n",
      "  Q = q_network.forward(torch.tensor([state]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test Code\n",
    "class DummyModule(nn.Module):\n",
    "    def forward(self, state):\n",
    "        return torch.FloatTensor([1, 2])  # Constant output\n",
    "\n",
    "\n",
    "# Sample 1000 actions\n",
    "_actions = [epsilon_greedy(np.array([1, 2, 3, 4]), DummyModule(), epsilon=0.2) for _ in range(1000)]\n",
    "\n",
    "sns.histplot(_actions, discrete=True, stat=\"density\")\n",
    "plt.xticks([0, 1])\n",
    "plt.show()\n",
    "\n",
    "_zeros = 1000 - sum(_actions)\n",
    "# Note: This is a stochastic test. It produces a false error in 1% of the cases\n",
    "assert 75 < _zeros < 125, f\"Frequency of action 0 ({_zeros}) is outside the 99% confidence interval [76, 124]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### c) Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mse1 = nn.MSELoss()\n",
    "import sys\n",
    "def compute_loss(q_network, target_network, states, actions, rewards, next_states, is_terminal, gamma=0.99):\n",
    "    # TODO Implement loss function\n",
    "    \n",
    "    qvals, expected_qvals, target_qvals = [],[],[]\n",
    "\n",
    "    for i, s, a, r, ns, ter in zip(range(len(states)), states, actions, rewards, next_states, is_terminal):      \n",
    "        # qvals.append(q_network.forward(s))\n",
    "        qvals.append(q_network.forward(s)[a])\n",
    "        \n",
    "        # sys.exit(0)\n",
    "        # print(type(q_network.forward(ns)))\n",
    "        na = torch.argmax(q_network.forward(ns))\n",
    "        # print(na)\n",
    "        # sys.exit(0)\n",
    "        target_qvals.append(target_network.forward(ns)[na])\n",
    "        # target_qvals.append(target_network.forward(ns))\n",
    "        # print(target_qvals)\n",
    "        # sys.exit(0)\n",
    "        if ter:\n",
    "            #qvals.append(0)\n",
    "            # target_qvals[-1] = torch.tensor(0.0,requires_grad = True)\n",
    "            expected_qvals.append(r)\n",
    "        else:\n",
    "            # print(target_qvals)\n",
    "            # sys.exit(0)\n",
    "            #expected_qvals.append(0)\n",
    "            expected_qvals.append(r+(gamma*target_qvals[-1]))\n",
    "    # print(qvals.shape,expected_qvals.shape)    \n",
    "    qvals = torch.tensor(qvals)\n",
    "    expected_qvals = torch.tensor(expected_qvals)\n",
    "    loss = mse1(qvals, expected_qvals.unsqueeze(1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "import sys\n",
    "def compute_loss(q_network, target_network, states, actions, rewards, next_states, is_terminal, gamma=0.99):\n",
    "    for ter in is_terminal:\n",
    "    # TODO Implement loss function\n",
    "        q_vals = q_network(states).gather(1,actions)\n",
    "        target_qvals = torch.zeros(len(states))\n",
    "        non_final_next_states = torch.cat([s for s in next_states if s is not ter])\n",
    "        if ter == False:\n",
    "            target_qvals = target_network(non_final_next_states).max(1)[0].detach()\n",
    "        expected_qvals = (target_qvals*gamma)+rewards\n",
    "   \n",
    "    # print(qvals.shape,expected_qvals.shape)    \n",
    "    qvals = torch.tensor(qvals)\n",
    "    expected_qvals = torch.tensor(expected_qvals)\n",
    "    loss = mse(qvals, expected_qvals.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### d) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/gym/core.py:200: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_8367/111603329.py:55: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  is_terminal = np.zeros(batch_size, dtype=np.bool)\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x256 and 4x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=44'>45</a>\u001b[0m terminal_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mBoolTensor(terminal_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=46'>47</a>\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=47'>48</a>\u001b[0m loss \u001b[39m=\u001b[39m compute_loss(q_network, target_network, state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, gamma)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=48'>49</a>\u001b[0m loss\u001b[39m.\u001b[39mrequires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=50'>51</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;32m/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb Cell 19\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(q_network, target_network, states, actions, rewards, next_states, is_terminal, gamma)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=7'>8</a>\u001b[0m     non_final_next_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([s \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m next_states \u001b[39mif\u001b[39;00m s \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m ter])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m ter \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=9'>10</a>\u001b[0m         target_qvals \u001b[39m=\u001b[39m target_network(non_final_next_states)\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=10'>11</a>\u001b[0m     expected_qvals \u001b[39m=\u001b[39m (target_qvals\u001b[39m*\u001b[39mgamma)\u001b[39m+\u001b[39mrewards\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=12'>13</a>\u001b[0m \u001b[39m# print(qvals.shape,expected_qvals.shape)    \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb Cell 19\u001b[0m in \u001b[0;36mDeepQNetwork.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=14'>15</a>\u001b[0m     \u001b[39m# TODO Implement forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden(state)) \u001b[39m# Different functions?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=16'>17</a>\u001b[0m     Q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000017?line=17'>18</a>\u001b[0m     \u001b[39m#Q = F.softmax(self.output(x),dim=1)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/rllbc_bpa2/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x256 and 4x128)"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epsilon = 0.05  # For epsilon greedy action sampling\n",
    "batch_size = 64\n",
    "NETWORK_UPDATE_FREQUENCY = 4\n",
    "NETWORK_SYNC_FREQUENCY = 20\n",
    "gamma = 0.99\n",
    "episodes = 100\n",
    "replay_buffer_size = 10000 #TODO\n",
    "env.seed(1234)\n",
    "q_network = DeepQNetwork()\n",
    "target_network = deepcopy(q_network)\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=1e-3)\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "step_count = 0\n",
    "total_rewards = []\n",
    "with tqdm(range(episodes)) as pbar:\n",
    "    for _ in pbar:\n",
    "        \n",
    "        state, done = env.reset(), False\n",
    "        # print(state)\n",
    "        # sys.exit(0)\n",
    "        rewards = []\n",
    "\n",
    "        while not done:\n",
    "            # Take a step\n",
    "            action = epsilon_greedy(state, q_network, epsilon=epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Bookkeeping\n",
    "            rewards.append(reward)\n",
    "            buffer.add(state=state, action=action, reward=reward, next_state=next_state, is_terminal=done and env._elapsed_steps < 500)\n",
    "            state = next_state\n",
    "\n",
    "            step_count += 1\n",
    "\n",
    "            # Update network every NETWORK_UPDATE_FREQUENCY steps\n",
    "            if step_count % NETWORK_UPDATE_FREQUENCY == 0:\n",
    "                # Sample batch of transitions\n",
    "                state_batch, action_batch, reward_batch, next_state_batch, terminal_batch = buffer.sample_batch(batch_size=batch_size)\n",
    "                state_batch = torch.FloatTensor(state_batch)\n",
    "                next_state_batch = torch.FloatTensor(next_state_batch)\n",
    "                action_batch = torch.LongTensor(action_batch).reshape(-1, 1)\n",
    "                reward_batch = torch.FloatTensor(reward_batch).reshape(-1, 1)\n",
    "                terminal_batch = torch.BoolTensor(terminal_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = compute_loss(q_network, target_network, state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, gamma)\n",
    "                loss.requires_grad=True\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "           \n",
    "            # Sync networks every NETWORK_SYNC_FREQUENCY steps\n",
    "            if step_count % NETWORK_SYNC_FREQUENCY == 0:\n",
    "                # ********************\n",
    "                # TODO Synchronize networks\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "                # ********************\n",
    "        # print(buffer.sample_batch(6))\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "         # Print statistics\n",
    "        pbar.set_description(f\"Mean training reward {np.mean(total_rewards[-100:]):.02f}\")\n",
    "        if np.mean(total_rewards[-100:]) == 500:\n",
    "            break # Stop training\n",
    "\n",
    "# Save model\n",
    "with open(\"solution/b2d.pt\", \"wb\") as f:\n",
    "    torch.save(q_network, f)\n",
    "\n",
    "# Plot training\n",
    "plt.plot(total_rewards, label=\"per episode\")\n",
    "plt.plot(pd.DataFrame(total_rewards).rolling(100).mean(), label=\"average reward\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"solution/b2d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 100/100 [00:00<00:00, 1515.78it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Average reward below 487.5, got 9.44",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000018?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m env\u001b[39m.\u001b[39m_elapsed_steps\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000018?line=15'>16</a>\u001b[0m _avg_reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([_rollout(seed\u001b[39m=\u001b[39mi) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidating\u001b[39m\u001b[39m\"\u001b[39m)])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000018?line=16'>17</a>\u001b[0m \u001b[39massert\u001b[39;00m _avg_reward \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m487.5\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAverage reward below 487.5, got \u001b[39m\u001b[39m{\u001b[39;00m_avg_reward\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shubham/rwth/suse22/RL/tasks_bpa2/tasks/dqn.ipynb#ch0000018?line=17'>18</a>\u001b[0m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mok (Average reward \u001b[39m\u001b[39m{\u001b[39;00m_avg_reward\u001b[39m:\u001b[39;00m\u001b[39m0.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Average reward below 487.5, got 9.44"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "policy = q_network\n",
    "policy.eval()  # Switch to evaluation mode\n",
    "\n",
    "\n",
    "def _rollout(seed):\n",
    "    env.reset(seed=seed)\n",
    "    state, done = env.reset(), False\n",
    "    while not done:\n",
    "        probs = policy(torch.tensor(state).float().reshape((1, -1)))[0]\n",
    "        action = np.argmax(probs.detach().numpy())  # Greedy action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "    return env._elapsed_steps\n",
    "\n",
    "\n",
    "_avg_reward = np.mean([_rollout(seed=i) for i in tqdm(range(100), desc=\"Validating\")])\n",
    "assert _avg_reward >= 487.5, f\"Average reward below 487.5, got {_avg_reward}\"\n",
    "f\"ok (Average reward {_avg_reward:0.2f})\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rllbc_bpa2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b12679fe4722524eeb4ab13231b185eda78a2ffc564a839f2b0a0233119771d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
